# worker/worker_tasks.py - MULTI-MODEL + GPU ACCELERATION
import os
import json
import subprocess
import logging
import uuid
import hashlib
from typing import Dict, Any, List, Optional
from datetime import datetime

import redis
from redis import Redis

import psycopg
from psycopg.rows import dict_row

import meilisearch
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# GPU Support
import torch

# ===== ENV =====
REDIS_URL = os.getenv("REDIS_URL", "redis://redis:6379/0")
RQ_QUEUE = os.getenv("RQ_QUEUE", "kb_ingestion")

POSTGRES_HOST = os.getenv("POSTGRES_HOST", "postgres")
POSTGRES_DB = os.getenv("POSTGRES_DB", "kb")
POSTGRES_USER = os.getenv("POSTGRES_USER", "kbuser")
POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "kbpass")

MEILI_URL = os.getenv("MEILI_URL", "http://meili:7700")
MEILI_MASTER_KEY = os.getenv("MEILI_MASTER_KEY", os.getenv("MEILI_KEY", "change_me_meili_key"))

QDRANT_URL = os.getenv("QDRANT_URL", "http://qdrant:6333")
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://ollama:11434")

KB_ROOT = os.getenv("KB_ROOT", "/mnt/kb")
MEILI_INDEX = "kb_docs"

Q_REDIS_KEY_PROGRESS = "kb:progress"
Q_REDIS_KEY_FAILED = "kb:failed_docs"
Q_REDIS_KEY_CURRENT_DOC = "kb:current_doc"
Q_REDIS_KEY_PROCESSING_LOG = "kb:processing_log"
Q_REDIS_KEY_STATS = "kb:stats"

# GPU Detection
DEVICE = None
GPU_AVAILABLE = False

try:
    if torch.cuda.is_available():
        DEVICE = 'cuda'
        GPU_AVAILABLE = True
        print(f"ðŸŽ® Worker GPU DETECTED: {torch.cuda.get_device_name(0)}")
        print(f"ðŸŽ® CUDA Version: {torch.version.cuda}")
        print(f"ðŸŽ® PyTorch Version: {torch.__version__}")
    else:
        DEVICE = 'cpu'
        print("ðŸ’» Worker usando CPU")
except Exception as e:
    DEVICE = 'cpu'
    print(f"âš ï¸ GPU detection error: {e}, usando CPU")

# Batch size ottimizzato
BATCH_SIZE = 64 if GPU_AVAILABLE else 16
print(f"ðŸ“¦ Worker batch size: {BATCH_SIZE} ({'GPU' if GPU_AVAILABLE else 'CPU'} optimized)")

# Configurazione modelli
MODEL_CONFIGS = {
    "sentence-transformer": {
        "name": "all-MiniLM-L6-v2",
        "dimension": 384,
        "collection_prefix": "kb_st",
        "type": "transformers"
    },
    "llama3": {
        "name": "llama3",
        "dimension": 4096,
        "collection_prefix": "kb_llama3",
        "type": "ollama"
    },
    "mistral": {
        "name": "mistral",
        "dimension": 4096,
        "collection_prefix": "kb_mistral",
        "type": "ollama"
    }
}

# Logger setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===== GLOBALS =====
_st_model = None
redis_conn = None


def get_redis():
    """Ottiene connessione Redis."""
    global redis_conn
    if redis_conn is None:
        redis_conn = redis.from_url(REDIS_URL, decode_responses=True)
    return redis_conn


def get_sentence_transformer_model():
    """Carica modello Sentence Transformer con GPU support."""
    global _st_model
    if _st_model is None:
        from sentence_transformers import SentenceTransformer
        model_name = MODEL_CONFIGS["sentence-transformer"]["name"]
        logger.info(f"Loading Sentence Transformer model: {model_name} on {DEVICE}")
        _st_model = SentenceTransformer(model_name, device=DEVICE)
        logger.info(f"âœ… Sentence Transformer loaded on {DEVICE}")
    return _st_model


def get_qdrant_client():
    """Ottiene client Qdrant."""
    return QdrantClient(url=QDRANT_URL)


def get_meili_client():
    """Ottiene client Meilisearch."""
    return meilisearch.Client(MEILI_URL, MEILI_MASTER_KEY)


def get_postgres_conn():
    """Ottiene connessione PostgreSQL."""
    conn_str = f"host={POSTGRES_HOST} dbname={POSTGRES_DB} user={POSTGRES_USER} password={POSTGRES_PASSWORD}"
    return psycopg.connect(conn_str, row_factory=dict_row)


def compute_file_hash(file_path: str) -> str:
    """Calcola hash SHA256 del file."""
    hasher = hashlib.sha256()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            hasher.update(chunk)
    return hasher.hexdigest()


def extract_metadata_from_path(file_path: str, kb_root: str) -> Dict[str, Any]:
    """Estrae metadati dal percorso del file."""
    rel_path = os.path.relpath(file_path, kb_root)
    parts = rel_path.split(os.sep)
    
    metadata = {
        "area": None,
        "anno": None,
        "cliente": None,
        "categoria": None,
        "tipo_doc": None,
        "codice_appalto": None,
        "oggetto": None,
    }
    
    # Area (primo livello con underscore)
    if parts and parts[0].startswith("_"):
        metadata["area"] = parts[0][1:]
    
    # Anno (pattern YYYY)
    for part in parts:
        if len(part) == 4 and part.isdigit():
            metadata["anno"] = part
            break
    
    # Codice appalto (pattern ASXXXX)
    for part in parts:
        if part.startswith("AS") and len(part) > 2:
            metadata["codice_appalto"] = part
            # Estrai cliente dopo underscore
            if "_" in part:
                metadata["cliente"] = part.split("_", 1)[1]
            break
    
    # Categoria e tipo doc da keywords
    filename_lower = os.path.basename(file_path).lower()
    
    categoria_keywords = {
        "Gestionale": ["amc", "hr", "logistica", "inventario", "magazzino"],
        "SanitÃ ": ["sio", "sia", "cce", "lis", "ris", "pacs", "ps", "cup", "cartella", "clinica"],
        "Territoriale": ["sit", "fse", "telemedicina"],
        "Analytics": ["dwh", "analytics", "bi"],
        "Compliance": ["gdpr", "privacy", "sicurezza"],
        "Emergenza": ["118", "emergenza"]
    }
    
    for cat, keywords in categoria_keywords.items():
        if any(kw in filename_lower or kw in rel_path.lower() for kw in keywords):
            metadata["categoria"] = cat
            break
    
    tipo_doc_keywords = {
        "Offerta Tecnica": ["offerta", "tecnica"],
        "Offerta Economica": ["economica", "prezzo"],
        "Documentazione": ["documentazione", "bando", "capitolato"],
        "Manuale": ["manuale", "guida"],
        "Specifiche": ["specifiche", "requisiti"],
    }
    
    for tipo, keywords in tipo_doc_keywords.items():
        if any(kw in filename_lower for kw in keywords):
            metadata["tipo_doc"] = tipo
            break
    
    return metadata


def extract_text_from_file(file_path: str) -> str:
    """Estrae testo dal file usando unstructured."""
    try:
        from unstructured.partition.auto import partition
        
        elements = partition(filename=file_path)
        text = "\n\n".join([str(el) for el in elements])
        return text.strip()
    except Exception as e:
        logger.error(f"Error extracting text from {file_path}: {e}")
        return ""


def generate_embedding_sentence_transformer(text: str) -> List[float]:
    """Genera embedding usando Sentence Transformer con GPU."""
    model = get_sentence_transformer_model()
    
    # Tronca testo se troppo lungo
    max_length = 512
    if len(text.split()) > max_length:
        text = " ".join(text.split()[:max_length])
    
    # Genera embedding (automaticamente usa GPU se disponibile)
    embedding = model.encode(text, convert_to_tensor=False, show_progress_bar=False)
    return embedding.tolist()


def generate_embedding_ollama(text: str, model_name: str) -> List[float]:
    """Genera embedding usando Ollama."""
    import requests
    
    # Tronca testo se troppo lungo
    max_length = 2048
    if len(text.split()) > max_length:
        text = " ".join(text.split()[:max_length])
    
    url = f"{OLLAMA_URL}/api/embeddings"
    payload = {
        "model": model_name,
        "prompt": text
    }
    
    try:
        response = requests.post(url, json=payload, timeout=60)
        response.raise_for_status()
        data = response.json()
        return data.get("embedding", [])
    except Exception as e:
        logger.error(f"Ollama embedding error for model {model_name}: {e}")
        return []


def ensure_qdrant_collection(collection_name: str, vector_size: int):
    """Crea collection Qdrant se non esiste."""
    client = get_qdrant_client()
    
    try:
        client.get_collection(collection_name)
        logger.info(f"Collection {collection_name} already exists")
    except Exception:
        logger.info(f"Creating collection {collection_name} with vector size {vector_size}")
        client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
        )


def process_document_incremental(file_path: str, model_name: str = "sentence-transformer") -> Dict[str, Any]:
    """Processa un singolo documento (modalitÃ  incremental)."""
    
    start_time = datetime.now()
    r = get_redis()
    
    # Update current doc
    r.set(Q_REDIS_KEY_CURRENT_DOC, file_path)
    
    result = {
        "file_path": file_path,
        "status": "success",
        "model": model_name,
        "processing_time_ms": 0,
        "error": None
    }
    
    try:
        # Verifica esistenza file
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")
        
        # Calcola hash
        file_hash = compute_file_hash(file_path)
        
        # Verifica se giÃ  processato
        conn = get_postgres_conn()
        with conn.cursor() as cur:
            cur.execute(
                "SELECT id FROM documents WHERE file_path = %s AND file_hash = %s AND model = %s",
                (file_path, file_hash, model_name)
            )
            existing = cur.fetchone()
            
            if existing:
                logger.info(f"Document already processed (hash match): {file_path}")
                result["status"] = "skipped"
                result["reason"] = "already_processed"
                conn.close()
                return result
        
        conn.close()
        
        # Estrai testo
        logger.info(f"Extracting text from: {file_path}")
        text = extract_text_from_file(file_path)
        
        if not text:
            raise ValueError("No text extracted from file")
        
        # Estrai metadati
        metadata = extract_metadata_from_path(file_path, KB_ROOT)
        
        # Genera embedding
        logger.info(f"Generating embedding with {model_name}...")
        
        model_config = MODEL_CONFIGS.get(model_name)
        if not model_config:
            raise ValueError(f"Unknown model: {model_name}")
        
        if model_config["type"] == "transformers":
            embedding = generate_embedding_sentence_transformer(text)
        elif model_config["type"] == "ollama":
            embedding = generate_embedding_ollama(text, model_config["name"])
        else:
            raise ValueError(f"Unknown model type: {model_config['type']}")
        
        if not embedding:
            raise ValueError("Failed to generate embedding")
        
        # Salva in PostgreSQL
        conn = get_postgres_conn()
        with conn.cursor() as cur:
            cur.execute("""
                INSERT INTO documents (
                    file_path, file_hash, text_content, model,
                    area, anno, cliente, categoria, tipo_doc, codice_appalto, oggetto
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                RETURNING id
            """, (
                file_path, file_hash, text, model_name,
                metadata.get("area"), metadata.get("anno"), metadata.get("cliente"),
                metadata.get("categoria"), metadata.get("tipo_doc"),
                metadata.get("codice_appalto"), metadata.get("oggetto")
            ))
            doc_id = cur.fetchone()["id"]
            conn.commit()
        conn.close()
        
        # Salva in Qdrant
        collection_name = f"{model_config['collection_prefix']}_docs"
        ensure_qdrant_collection(collection_name, model_config["dimension"])
        
        client = get_qdrant_client()
        point = PointStruct(
            id=str(uuid.uuid4()),
            vector=embedding,
            payload={
                "doc_id": doc_id,
                "file_path": file_path,
                "text": text[:500],  # Solo preview
                **metadata
            }
        )
        client.upsert(collection_name=collection_name, points=[point])
        
        # Salva in Meilisearch
        meili = get_meili_client()
        try:
            meili.index(MEILI_INDEX).add_documents([{
                "id": str(doc_id),
                "file_path": file_path,
                "text_content": text,
                **metadata
            }])
        except Exception as e:
            logger.warning(f"Meilisearch indexing failed: {e}")
        
        # Update stats
        processing_time = (datetime.now() - start_time).total_seconds() * 1000
        result["processing_time_ms"] = processing_time
        
        # Log processing
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "file_path": file_path,
            "model": model_name,
            "status": "success",
            "processing_time_ms": processing_time,
            "device": DEVICE
        }
        r.lpush(Q_REDIS_KEY_PROCESSING_LOG, json.dumps(log_entry))
        r.ltrim(Q_REDIS_KEY_PROCESSING_LOG, 0, 999)  # Keep last 1000
        
        logger.info(f"âœ… Document processed in {processing_time:.2f}ms: {file_path}")
        
    except Exception as e:
        result["status"] = "failed"
        result["error"] = str(e)
        
        # Log failure
        r.lpush(Q_REDIS_KEY_FAILED, json.dumps({
            "timestamp": datetime.now().isoformat(),
            "file_path": file_path,
            "model": model_name,
            "error": str(e)
        }))
        r.ltrim(Q_REDIS_KEY_FAILED, 0, 99)  # Keep last 100
        
        logger.error(f"âŒ Failed to process {file_path}: {e}")
    
    return result


def process_batch_full_rescan(file_paths: List[str], model_name: str = "sentence-transformer") -> Dict[str, Any]:
    """Processa batch di documenti (modalitÃ  full rescan)."""
    
    start_time = datetime.now()
    r = get_redis()
    
    results = {
        "total": len(file_paths),
        "processed": 0,
        "skipped": 0,
        "failed": 0,
        "processing_time_ms": 0
    }
    
    for i, file_path in enumerate(file_paths, 1):
        # Update progress
        progress = {
            "current": i,
            "total": len(file_paths),
            "percentage": (i / len(file_paths)) * 100,
            "current_file": file_path
        }
        r.set(Q_REDIS_KEY_PROGRESS, json.dumps(progress))
        
        # Process document
        result = process_document_incremental(file_path, model_name)
        
        if result["status"] == "success":
            results["processed"] += 1
        elif result["status"] == "skipped":
            results["skipped"] += 1
        else:
            results["failed"] += 1
    
    results["processing_time_ms"] = (datetime.now() - start_time).total_seconds() * 1000
    
    # Clear progress
    r.delete(Q_REDIS_KEY_PROGRESS)
    r.delete(Q_REDIS_KEY_CURRENT_DOC)
    
    logger.info(f"âœ… Batch completed: {results}")
    return results


def task_ingest_documents(mode: str = "incremental", model: str = "sentence-transformer"):
    """
    Task principale per ingestion documenti.
    
    Args:
        mode: 'incremental' o 'full_rescan'
        model: 'sentence-transformer', 'llama3', o 'mistral'
    """
    
    logger.info(f"ðŸš€ Starting ingestion: mode={mode}, model={model}, device={DEVICE}")
    
    # Scan filesystem
    file_paths = []
    for root, dirs, files in os.walk(KB_ROOT):
        # Skip hidden directories
        dirs[:] = [d for d in dirs if not d.startswith('.')]
        
        for file in files:
            if file.startswith('.'):
                continue
            
            ext = os.path.splitext(file)[1].lower()
            if ext in ['.pdf', '.docx', '.doc', '.pptx', '.txt', '.html', '.md']:
                file_paths.append(os.path.join(root, file))
    
    logger.info(f"Found {len(file_paths)} documents to process")
    
    if mode == "incremental":
        # Process one by one
        for file_path in file_paths:
            process_document_incremental(file_path, model)
    
    elif mode == "full_rescan":
        # Process in batches
        batch_size = BATCH_SIZE
        for i in range(0, len(file_paths), batch_size):
            batch = file_paths[i:i+batch_size]
            process_batch_full_rescan(batch, model)
    
    logger.info("âœ… Ingestion completed")


if __name__ == "__main__":
    # Test
    print("ðŸ§ª Testing worker setup...")
    print(f"Device: {DEVICE}")
    print(f"GPU Available: {GPU_AVAILABLE}")
    print(f"Batch Size: {BATCH_SIZE}")
    
    if GPU_AVAILABLE:
        print(f"GPU Name: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
